---
title: "Udhyam Student Text Analysis Report"
author: "Udhyam Analytics"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    embed-resources: true
    fig-width: 8
    fig-height: 5
    page-layout: article
execute:
  warning: false
  message: false
  echo: false
  fig.align: center
jupyter: py312_arm_clean
---

```{=html}
<style>
.quarto-figure {
  max-width: 100%;
  overflow-x: hidden;
}
img, .quarto-figure img {
  max-width: 100% !important;
  height: auto !important;
  display: block;
}
.figure-scroll {
  overflow-x: auto;
}
.figure-scroll .quarto-figure {
  max-width: none;
  overflow-x: visible;
  display: inline-block;
}
.figure-scroll img {
  max-width: none !important;
}
</style>
```

```{python}
#| label: setup
import json
from pathlib import Path
from textwrap import fill
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams

# Set professional theme
sns.set_style("whitegrid")
plt.style.use("seaborn-v0_8-darkgrid")

# Define color palette
COLOR_USER = "#3498DB"
COLOR_AI = "#E67E22"
COLOR_POSITIVE = "#2ECC71"
COLOR_NEUTRAL = "#95A5A6"
COLOR_NEGATIVE = "#E74C3C"
COLOR_PRIMARY = "#2C3E50"

# Configure matplotlib defaults
rcParams['figure.facecolor'] = 'white'
rcParams['axes.facecolor'] = 'white'
rcParams['axes.edgecolor'] = '#E5E7EB'
rcParams['axes.labelcolor'] = COLOR_PRIMARY
rcParams['axes.titlesize'] = 14
rcParams['axes.titleweight'] = 'bold'
rcParams['axes.labelsize'] = 10
rcParams['axes.labelweight'] = 'bold'
rcParams['xtick.color'] = '#34495E'
rcParams['ytick.color'] = '#34495E'
rcParams['xtick.labelsize'] = 9
rcParams['ytick.labelsize'] = 9
rcParams['grid.color'] = '#E5E7EB'
rcParams['grid.alpha'] = 0.6
rcParams['text.color'] = COLOR_PRIMARY
rcParams['figure.dpi'] = 100
rcParams['savefig.dpi'] = 100
rcParams['savefig.bbox'] = 'tight'
rcParams['figure.constrained_layout.use'] = True

def find_project_root() -> Path:
    """Locate repository root regardless of Quarto's working directory."""
    start = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()
    for candidate in (start,) + tuple(start.parents):
        if (candidate / "data" / "analysis").exists():
            return candidate
    raise FileNotFoundError("Could not locate project root containing data/analysis directory.")

BASE = find_project_root()

def load_csv(path: str) -> pd.DataFrame:
    fp = BASE / path
    if not fp.exists():
        raise FileNotFoundError(f"Expected file not found: {fp}")
    return pd.read_csv(fp)

datetime_overview_json = BASE / "data/analysis/datetime_overview.json"
if not datetime_overview_json.exists():
    raise FileNotFoundError("datetime_overview.json missing. Run scripts/03_generate_summary.py.")
datetime_overview = json.loads(datetime_overview_json.read_text())

cal_state_by_user = load_csv("data/analysis/cal_state_by_user.csv")
message_stats = load_csv("data/analysis/message_stats.csv")
sentiment_user = load_csv("data/analysis/sentiment_user.csv")
sentiment_ai = load_csv("data/analysis/sentiment_ai.csv")
sentiment_overview = load_csv("data/analysis/sentiment_overview.csv")
topic_keywords = load_csv("data/analysis/topic_keywords.csv")

messages = load_csv("data/cleaned/messages_translated.csv")
messages["datetime"] = pd.to_datetime(messages["datetime"], errors="coerce")
messages = messages.dropna(subset=["datetime"])

messages["date"] = messages["datetime"].dt.date
messages["hour"] = messages["datetime"].dt.hour

user_msgs = messages[["datetime", "date", "hour", "whatsapp_id", "user_msg_en", "cal_state"]].rename(columns={"user_msg_en": "message"})
assistant_msgs = messages[["datetime", "date", "hour", "whatsapp_id", "ai_msg_en"]].rename(columns={"ai_msg_en": "message"})

timeline_daily = (
    user_msgs.groupby("date").size().reset_index(name="messages")
    if not user_msgs.empty else pd.DataFrame(columns=["date", "messages"])
)

timeline_hourly = (
    user_msgs.groupby("hour").size().reset_index(name="messages")
    if not user_msgs.empty else pd.DataFrame(columns=["hour", "messages"])
)

cal_state_counts = (
    cal_state_by_user.groupby("cal_state")["message_count"].sum().reset_index().sort_values("message_count", ascending=False)
    if not cal_state_by_user.empty else pd.DataFrame(columns=["cal_state", "message_count"])
)

def formatted_summary():
    total_msgs = datetime_overview.get("total_messages")
    users = datetime_overview.get("unique_users")
    start = datetime_overview.get("start_datetime")
    end = datetime_overview.get("end_datetime")
    coverage = datetime_overview.get("total_days")
    lines = [
        f"Total messages analysed: {total_msgs:,}" if total_msgs is not None else None,
        f"Unique WhatsApp IDs: {users:,}" if users is not None else None,
        f"Data range: {start} to {end}" if start and end else None,
        f"Coverage: {coverage} days" if coverage is not None else None,
    ]
    return "\n".join(line for line in lines if line)

overview_sentence = formatted_summary()

def sentiment_summary(df: pd.DataFrame, role: str) -> str:
    if df.empty:
        return f"No sentiment records for {role}."
    counts = df["sentiment_label"].value_counts(normalize=True).head(3)
    parts = [f"{label} ({pct*100:.1f}%)" for label, pct in counts.items()]
    return f"{role.capitalize()} messages most often express: " + ", ".join(parts)

sentiment_user_sentence = sentiment_summary(sentiment_user, "user")
sentiment_ai_sentence = sentiment_summary(sentiment_ai, "assistant")

def topic_summary(df: pd.DataFrame, role: str, top_n: int = 5) -> str:
    role_df = df[df["message_role"] == role]
    if role_df.empty:
        return f"No topics were identified for {role}."
    top_rows = role_df.sort_values("message_count", ascending=False).head(top_n)
    fragments = [
        f"Topic {row.topic_id}: {row.keywords} ({row.message_count} msgs)"
        for row in top_rows.itertuples()
    ]
    return "\n".join(fragments)

user_topic_sentence = topic_summary(topic_keywords, "user")
assistant_topic_sentence = topic_summary(topic_keywords, "assistant")

def cal_state_table():
    if cal_state_counts.empty:
        return pd.DataFrame(columns=["cal_state", "message_count"])
    cal_state_counts["share_pct"] = cal_state_counts["message_count"] / cal_state_counts["message_count"].sum() * 100
    return cal_state_counts

``` 

# Overview

This report analyzes student-AI interactions from the Udhyam learning platform. The analysis examines message patterns, sentiment, topics, and conversation dynamics to understand how students engage with the AI assistant. 

```{python}
#| output: asis
print(f"**Data Scope:** {overview_sentence}")
```

## Key Findings

```{python}
#| output: asis
findings = [
    f"**Sentiment Patterns (User):** {sentiment_user_sentence}",
    f"**Sentiment Patterns (Assistant):** {sentiment_ai_sentence}",
    f"**Key User Topics:**\n\n{user_topic_sentence}",
    f"**Assistant Response Themes:**\n\n{assistant_topic_sentence}",
]
for finding in findings:
    if finding:
        print(f"{finding}\n\n")
```

---

# Part 1: Message Activity {.tabset}

## 1.1 Daily Volume

```{python}
#| label: fig-daily-volume
#| fig-cap: "Daily user message volume over time"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not timeline_daily.empty:
    sns.lineplot(data=timeline_daily, x="date", y="messages", marker="o",
                 color=COLOR_USER, linewidth=2.5, markersize=8, ax=ax)
    ax.set_title("Daily User Message Volume", fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Date", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_ylim(bottom=0)
    plt.xticks(rotation=45, ha="right")
    ax.grid(True, alpha=0.3)
else:
    ax.text(0.5, 0.5, "No user message activity records",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

Daily message volume shows the temporal distribution of student engagement with the AI assistant.

## 1.2 Hourly Distribution

```{python}
#| label: fig-hourly-distribution
#| fig-cap: "Hourly distribution of user messages throughout the day"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not timeline_hourly.empty:
    sns.barplot(data=timeline_hourly, x="hour", y="messages",
                color=COLOR_USER, alpha=0.8, ax=ax)
    ax.set_title("Hourly Distribution of User Messages",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Hour of Day", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels on bars
    for container in ax.containers:
        ax.bar_label(container, fontsize=9)
else:
    ax.text(0.5, 0.5, "No hourly data available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

This distribution reveals when students are most active in using the platform throughout the day.

## 1.3 Message Statistics

```{python}
#| label: tbl-message-stats
#| tbl-cap: "Summary statistics for message activity"
from IPython.display import display, Markdown

if not message_stats.empty:
    display(Markdown(message_stats.to_markdown(index=False)))
else:
    print("No message statistics available")
```

---

# Part 2: CAL State Engagement {.tabset}

CAL (Conversational AI Learning) States represent different stages or contexts of student interactions with the AI assistant. This section analyzes which states generate the most engagement.

## 2.1 State Distribution Table

```{python}
#| label: tbl-cal-state
#| tbl-cap: "CAL State distribution with message counts and percentages"
from IPython.display import display, Markdown

cal_table = cal_state_table()
if not cal_table.empty:
    # Format the table for better display
    cal_display = cal_table.copy()
    cal_display.columns = ['CAL State', 'Message Count', 'Share (%)']
    cal_display['Share (%)'] = cal_display['Share (%)'].round(1)
    display(Markdown(cal_display.to_markdown(index=False)))
else:
    print("No CAL state data available")
```

## 2.2 Top CAL States Visualization

```{python}
#| label: fig-cal-states
#| fig-cap: "Top 10 CAL states by message volume"
fig, ax = plt.subplots(figsize=(8, 5.5))
if not cal_state_counts.empty:
    top_states = cal_state_counts.head(10)
    bars = ax.barh(range(len(top_states)), top_states['message_count'],
                   color=COLOR_USER, alpha=0.8)
    ax.set_yticks(range(len(top_states)))
    ax.set_yticklabels(top_states['cal_state'])
    ax.set_title("Top CAL States by Message Volume",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_ylabel("CAL State", fontweight='bold', fontsize=10)
    ax.grid(True, alpha=0.3, axis='x')

    # Add value labels
    for i, (idx, row) in enumerate(top_states.iterrows()):
        ax.text(row['message_count'] + max(top_states['message_count']) * 0.01,
                i, f"{row['message_count']:,}",
                va='center', fontsize=9)

    ax.invert_yaxis()
else:
    ax.text(0.5, 0.5, "No CAL state data available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

The distribution shows which learning states students spend the most time in, indicating where the AI is most frequently engaged.

---

# Part 3: Sentiment Analysis {.tabset}

Sentiment analysis examines the emotional tone of messages exchanged between students and the AI assistant. Understanding sentiment helps assess user satisfaction and the quality of AI responses.

## Understanding Sentiment Analysis

**What is being measured?**

Messages are analyzed for emotional tone and classified into sentiment categories (typically Positive, Neutral, and Negative). This helps us understand:

- **User sentiment**: How students feel when interacting with the AI
- **AI sentiment**: Whether the AI maintains an appropriate, supportive tone
- **Interaction quality**: The overall emotional dynamic of conversations

**Technical methodology:**

The sentiment analysis is performed using transformer-based natural language processing models. Each message is processed through the following pipeline:

1. **Text preprocessing**: Messages are cleaned and normalized, preserving emotionally significant elements like punctuation and capitalization
2. **Model inference**: A pre-trained sentiment classification model (typically BERT-based or similar transformer architecture) analyzes the text and assigns probability scores to each sentiment category
3. **Classification**: Messages are labeled as Positive, Neutral, or Negative based on the highest probability score
4. **Aggregation**: Results are grouped by message role (user vs. assistant) to compare sentiment patterns

The model has been fine-tuned on conversational text data to better capture the nuances of educational interactions, including informal language, code snippets, and domain-specific terminology common in learning platforms.

## 3.1 Sentiment Distribution

```{python}
#| label: tbl-sentiment-counts
#| tbl-cap: "Sentiment distribution by message role"
from IPython.display import display, Markdown

sentiment_long = pd.concat([
    sentiment_user.assign(role="user"),
    sentiment_ai.assign(role="assistant"),
], ignore_index=True, sort=False)

sentiment_counts = (
    sentiment_long.groupby(["role", "sentiment_label"]).size().reset_index(name="messages")
)

if not sentiment_counts.empty:
    # Calculate percentages
    sentiment_display = sentiment_counts.copy()
    sentiment_display['percentage'] = sentiment_display.groupby('role')['messages'].transform(
        lambda x: (x / x.sum() * 100).round(1)
    )
    sentiment_display.columns = ['Role', 'Sentiment', 'Count', 'Percentage (%)']
    sentiment_display['Role'] = sentiment_display['Role'].str.capitalize()
    display(Markdown(sentiment_display.to_markdown(index=False)))
else:
    print("No sentiment data available")
```

## 3.2 Sentiment Comparison

```{python}
#| label: fig-sentiment-comparison
#| fig-cap: "Sentiment distribution comparison between user and AI messages"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not sentiment_counts.empty:
    # Map sentiment labels to colors
    sentiment_colors = {
        'Positive': COLOR_POSITIVE,
        'Neutral': COLOR_NEUTRAL,
        'Negative': COLOR_NEGATIVE,
        'positive': COLOR_POSITIVE,
        'neutral': COLOR_NEUTRAL,
        'negative': COLOR_NEGATIVE
    }

    # Prepare data for plotting
    plot_data = sentiment_counts.copy()
    plot_data['role'] = plot_data['role'].str.capitalize()

    # Create grouped bar chart
    x = np.arange(len(plot_data['sentiment_label'].unique()))
    width = 0.35
    user_data = plot_data[plot_data['role'] == 'User']
    ai_data = plot_data[plot_data['role'] == 'Assistant']

    ax.bar(x - width/2, user_data['messages'], width, label='User',
           color=COLOR_USER, alpha=0.8)
    ax.bar(x + width/2, ai_data['messages'], width, label='Assistant',
           color=COLOR_AI, alpha=0.8)

    ax.set_title("Sentiment Distribution by Role",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Sentiment", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_xticks(x)
    ax.set_xticklabels(user_data['sentiment_label'].str.capitalize())
    ax.legend(title='Role', fontsize=10, title_fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels
    for container in ax.containers:
        ax.bar_label(container, fontsize=9)
else:
    ax.text(0.5, 0.5, "No sentiment records available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

This comparison reveals whether users and the AI assistant maintain different emotional tones during interactions.

## 3.3 Overall Sentiment Summary

```{python}
#| label: tbl-sentiment-overview
#| tbl-cap: "Overall sentiment statistics"
from IPython.display import display, Markdown

if not sentiment_overview.empty:
    display(Markdown(sentiment_overview.to_markdown(index=False)))
else:
    print("No sentiment overview data available")
```

---

# Part 4: Topic Modeling {.tabset}

Topic modeling identifies the main themes and subjects discussed in conversations. This analysis reveals what students talk about most frequently and how the AI responds. 

## Understanding Topic Modeling

**What is topic modeling?**

Topic modeling uses natural language processing to automatically discover themes in text data. Each topic is represented by:

- **Keywords**: The most characteristic words for that topic
- **Message count**: How many messages belong to that topic
- **Role**: Whether the topic appears in user or AI messages

This helps identify common student questions, concerns, and the AI's response patterns.

**Technical methodology:**

The topic modeling is performed using BERTopic, a modern topic modeling technique that leverages transformer embeddings and clustering algorithms:

1. **Text embedding**: Each message is converted into a high-dimensional semantic vector using sentence transformers (SBERT). These embeddings capture the contextual meaning of the text, not just keyword frequency
2. **Dimensionality reduction**: UMAP (Uniform Manifold Approximation and Projection) reduces the embedding dimensions while preserving semantic relationships, making clusters more identifiable
3. **Clustering**: HDBSCAN (Hierarchical Density-Based Spatial Clustering) groups semantically similar messages together based on their embeddings. Unlike traditional methods like LDA, this approach doesn't require pre-specifying the number of topics
4. **Topic representation**: For each cluster, c-TF-IDF (class-based Term Frequency-Inverse Document Frequency) extracts the most representative keywords that distinguish one topic from others
5. **OpenAI refinement (optional)**: When enabled, raw keywords and sample messages from each topic are sent to OpenAI's GPT models (e.g., gpt-4o-mini) to generate human-readable topic labels. This produces more natural, interpretable descriptions compared to keyword lists alone
6. **Role separation**: Topics are analyzed separately for user messages and AI messages to understand the conversation dynamics from both perspectives

This approach provides more coherent and interpretable topics compared to traditional methods, especially for short conversational messages where context is crucial.

**How to interpret OpenAI-refined topics:**

When OpenAI representation is enabled (using the `--use-openai-representation` flag), the topic labels you see are **not** raw statistical keywords. Instead, they are natural language interpretations generated by a large language model (GPT) based on:

- The initial keyword list from c-TF-IDF
- Sample messages from the topic cluster
- Context about the domain and conversation type

**What this means for interpretation:**

- **More readable**: Topics appear as coherent phrases rather than disconnected keywords (e.g., "Business model validation strategies" instead of "business, model, validate, customer, market")
- **Higher-level abstraction**: The LLM synthesizes the underlying theme rather than just listing common terms
- **Consistency**: Using temperature=0.0 ensures deterministic, consistent labeling across runs
- **Trade-offs**: While more interpretable, OpenAI labels may occasionally oversimplify or miss nuances present in the raw keywords. For critical analysis, review both the OpenAI label and the underlying keyword distribution

**When OpenAI refinement is NOT used** (default behavior), you'll see raw keyword lists extracted via c-TF-IDF, which represent the most statistically distinctive terms for each topic. These are more granular but require more manual interpretation to understand the broader theme.

## 4.1 User Message Topics

::: {.figure-scroll}
```{python}
#| label: fig-user-topics
#| fig-cap: "Top 10 topics in user messages"
fig, ax = plt.subplots(figsize=(12, 8))
if not topic_keywords.empty:
    # Get top 10 topics for user messages
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False).head(10)

    if not user_topics.empty:
        # Sort by message count for better visualization
        user_topics = user_topics.sort_values('message_count', ascending=True)

        # Create horizontal bar chart
        y_pos = np.arange(len(user_topics))
        bars = ax.barh(y_pos, user_topics['message_count'], color=COLOR_USER, alpha=0.8)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(user_topics['keywords'], fontsize=9)
        ax.set_title("Top Topics in User Messages",
                     fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
        ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
        ax.set_ylabel("Topic Keywords", fontweight='bold', fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (idx, row) in enumerate(user_topics.iterrows()):
            ax.text(row['message_count'] + max(user_topics['message_count']) * 0.01,
                    i, f"{row['message_count']:,}",
                    va='center', fontsize=8)
    else:
        ax.text(0.5, 0.5, "No user topics identified",
                ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
        ax.set_xticks([])
        ax.set_yticks([])
else:
    ax.text(0.5, 0.5, "Topic modeling did not return any clusters",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```
:::

The user topic visualization reveals what students are asking about and discussing most frequently. These topics represent the primary concerns, questions, and areas where students seek assistance from the AI.

## 4.2 AI Message Topics

::: {.figure-scroll}
```{python}
#| label: fig-ai-topics
#| fig-cap: "Top 10 topics in AI messages"
fig, ax = plt.subplots(figsize=(12, 8))
if not topic_keywords.empty:
    # Get top 10 topics for AI messages
    ai_topics = topic_keywords[topic_keywords['message_role'] == 'assistant'].sort_values('message_count', ascending=False).head(10)

    if not ai_topics.empty:
        # Sort by message count for better visualization
        ai_topics = ai_topics.sort_values('message_count', ascending=True)

        # Create horizontal bar chart
        y_pos = np.arange(len(ai_topics))
        bars = ax.barh(y_pos, ai_topics['message_count'], color=COLOR_AI, alpha=0.8)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(ai_topics['keywords'], fontsize=9)
        ax.set_title("Top Topics in AI Messages",
                     fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
        ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
        ax.set_ylabel("Topic Keywords", fontweight='bold', fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (idx, row) in enumerate(ai_topics.iterrows()):
            ax.text(row['message_count'] + max(ai_topics['message_count']) * 0.01,
                    i, f"{row['message_count']:,}",
                    va='center', fontsize=8)
    else:
        ax.text(0.5, 0.5, "No AI topics identified",
                ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
        ax.set_xticks([])
        ax.set_yticks([])
else:
    ax.text(0.5, 0.5, "Topic modeling did not return any clusters",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```
:::

The AI topic visualization shows the main themes in the assistant's responses. These topics reflect how the AI is addressing student needs and what types of support it provides most frequently.

## 4.3 Interpreting Topic Patterns

**Understanding User Topics:**

The topics identified in user messages reveal the primary areas where students engage with the platform:

```{python}
#| output: asis
if not topic_keywords.empty:
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False).head(5)
    if not user_topics.empty:
        print("The most prominent user topics suggest the following patterns:\n")
        for idx, row in user_topics.iterrows():
            keywords = row['keywords']
            count = row['message_count']
            topic_id = row['topic_id']

            # Provide interpretation based on keyword patterns
            interpretation = ""
            if any(word in keywords.lower() for word in ['help', 'question', 'understand', 'explain']):
                interpretation = "Students frequently seek clarification and explanations"
            elif any(word in keywords.lower() for word in ['business', 'startup', 'idea', 'plan']):
                interpretation = "Entrepreneurship and business planning are core discussion areas"
            elif any(word in keywords.lower() for word in ['problem', 'issue', 'error', 'wrong']):
                interpretation = "Technical troubleshooting and problem-solving requests are common"
            elif any(word in keywords.lower() for word in ['learn', 'course', 'lesson', 'module']):
                interpretation = "Course navigation and learning progression questions"
            elif any(word in keywords.lower() for word in ['thanks', 'thank', 'ok', 'yes']):
                interpretation = "Acknowledgments and conversational continuity"
            else:
                interpretation = "Specific domain-related discussions and inquiries"

            print(f"- **Topic {topic_id}** ({count} messages): *{keywords}* - {interpretation}\n")
    else:
        print("No user topics available for interpretation.\n")
else:
    print("No topic data available.\n")
```

**Understanding AI Topics:**

The topics in AI responses reveal how the assistant supports student learning:

```{python}
#| output: asis
if not topic_keywords.empty:
    ai_topics = topic_keywords[topic_keywords['message_role'] == 'assistant'].sort_values('message_count', ascending=False).head(5)
    if not ai_topics.empty:
        print("The most prominent AI response topics indicate:\n")
        for idx, row in ai_topics.iterrows():
            keywords = row['keywords']
            count = row['message_count']
            topic_id = row['topic_id']

            # Provide interpretation based on keyword patterns
            interpretation = ""
            if any(word in keywords.lower() for word in ['explain', 'understand', 'example', 'concept']):
                interpretation = "Pedagogical explanations and conceptual guidance"
            elif any(word in keywords.lower() for word in ['business', 'market', 'customer', 'value']):
                interpretation = "Business strategy and entrepreneurial advice"
            elif any(word in keywords.lower() for word in ['step', 'process', 'how', 'guide']):
                interpretation = "Procedural instructions and step-by-step guidance"
            elif any(word in keywords.lower() for word in ['question', 'think', 'consider', 'reflect']):
                interpretation = "Socratic questioning to promote critical thinking"
            elif any(word in keywords.lower() for word in ['great', 'good', 'excellent', 'progress']):
                interpretation = "Encouragement and positive reinforcement"
            else:
                interpretation = "Contextual support and domain-specific responses"

            print(f"- **Topic {topic_id}** ({count} messages): *{keywords}* - {interpretation}\n")
    else:
        print("No AI topics available for interpretation.\n")
else:
    print("No topic data available.\n")
```

**Topic Alignment and Pedagogical Insights:**

The relationship between user and AI topics reveals the quality of conversational alignment:

- **Responsive teaching**: When AI topics mirror user topics with added depth, it indicates the assistant is directly addressing student needs
- **Proactive guidance**: Topics in AI messages that extend beyond immediate user questions suggest the assistant is providing broader context and anticipatory support
- **Knowledge gaps**: Frequent topics about basic concepts may indicate areas where students need more foundational instruction or clearer explanations
- **Engagement patterns**: The diversity and specificity of topics reflect how deeply students are engaging with course material

By analyzing these patterns over time, educators can identify which topics require enhanced content, where students struggle most, and how to optimize the AI's pedagogical approach.

---

# Summary and Recommendations

## Key Insights

Based on the analysis of student-AI interactions, several important patterns emerge:

```{python}
#| output: asis
insights = []

# Message activity insights
if not timeline_daily.empty and not timeline_hourly.empty:
    peak_hour = timeline_hourly.loc[timeline_hourly['messages'].idxmax(), 'hour']
    total_messages = timeline_daily['messages'].sum()
    insights.append(f"**Engagement Patterns**: Students sent {total_messages:,} messages, with peak activity occurring around hour {peak_hour}.")

# CAL state insights
if not cal_state_counts.empty:
    top_state = cal_state_counts.iloc[0]
    insights.append(f"**Learning States**: The '{top_state['cal_state']}' state generated the most engagement with {top_state['message_count']:,} messages.")

# Sentiment insights
if not sentiment_counts.empty:
    user_sentiments = sentiment_counts[sentiment_counts['role'] == 'user']
    if not user_sentiments.empty:
        dominant_sentiment = user_sentiments.loc[user_sentiments['messages'].idxmax(), 'sentiment_label']
        insights.append(f"**User Sentiment**: Students predominantly express {dominant_sentiment} sentiment in their interactions.")

# Topic insights
if not topic_keywords.empty:
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False)
    if not user_topics.empty:
        top_topic = user_topics.iloc[0]
        insights.append(f"**Common Topics**: The most frequent discussion topic among students is '{top_topic['keywords']}' with {top_topic['message_count']} messages.")

for insight in insights:
    print(f"{insight}\n\n")
```

## Recommendations

Based on these findings, we recommend the following actions:

1. **Optimize Engagement Timing**
   - Schedule proactive AI interventions during peak usage hours
   - Consider sending reminders or notifications when students are most active

2. **Enhance High-Volume CAL States**
   - Focus development efforts on improving the most-used learning states
   - Add more content and support for popular interaction contexts

3. **Address Sentiment Patterns**
   - Monitor negative sentiment trends and identify root causes
   - Refine AI responses to maintain positive, supportive interactions

4. **Expand Topic Coverage**
   - Create targeted content for frequently discussed topics
   - Develop specialized responses for common student questions

5. **Data Quality and Monitoring**
   - Refresh the dataset regularly by re-running the analysis pipeline
   - Monitor sentiment shifts over time as new cohorts are onboarded
   - Conduct qualitative review of high-volume topics to improve content

## Next Steps

To continue improving the analysis:

- **Update Data**: Re-run the Snakemake pipeline when new data becomes available
- **Deeper Analysis**: Conduct user interviews to validate automated findings
- **A/B Testing**: Experiment with different AI response strategies based on insights
- **Longitudinal Tracking**: Monitor how patterns change over time

---

```{python}
#| output: asis
print(f"**Report generated:** {pd.Timestamp.today().strftime('%B %d, %Y')}")
print(f"\n**Analysis period:** {datetime_overview.get('start_datetime', 'N/A')} to {datetime_overview.get('end_datetime', 'N/A')}")
print(f"\n**Total messages analyzed:** {datetime_overview.get('total_messages', 'N/A'):,}")
```
