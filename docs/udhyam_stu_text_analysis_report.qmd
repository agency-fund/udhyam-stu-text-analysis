---
title: "Udhyam Student Text Analysis Report"
author: "Udhyam Analytics"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    embed-resources: true
    fig-width: 8
    fig-height: 5
    page-layout: article
execute:
  warning: false
  message: false
  echo: false
  fig.align: center
jupyter: py312_arm_clean
---

```{=html}
<style>
.quarto-figure {
  max-width: 100%;
  overflow-x: hidden;
}
img, .quarto-figure img {
  max-width: 100% !important;
  height: auto !important;
  display: block;
}
.figure-scroll {
  overflow-x: auto;
}
.figure-scroll .quarto-figure {
  max-width: none;
  overflow-x: visible;
  display: inline-block;
}
.figure-scroll img {
  max-width: none !important;
}
</style>
```

```{python}
#| label: setup
import json
import re
from pathlib import Path
from textwrap import fill
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams

# Set professional theme
sns.set_style("whitegrid")
plt.style.use("seaborn-v0_8-darkgrid")

# Define color palette
COLOR_USER = "#3498DB"
COLOR_AI = "#E67E22"
COLOR_POSITIVE = "#2ECC71"
COLOR_NEUTRAL = "#95A5A6"
COLOR_NEGATIVE = "#E74C3C"
COLOR_PRIMARY = "#2C3E50"

# Configure matplotlib defaults
rcParams['figure.facecolor'] = 'white'
rcParams['axes.facecolor'] = 'white'
rcParams['axes.edgecolor'] = '#E5E7EB'
rcParams['axes.labelcolor'] = COLOR_PRIMARY
rcParams['axes.titlesize'] = 14
rcParams['axes.titleweight'] = 'bold'
rcParams['axes.labelsize'] = 10
rcParams['axes.labelweight'] = 'bold'
rcParams['xtick.color'] = '#34495E'
rcParams['ytick.color'] = '#34495E'
rcParams['xtick.labelsize'] = 9
rcParams['ytick.labelsize'] = 9
rcParams['grid.color'] = '#E5E7EB'
rcParams['grid.alpha'] = 0.6
rcParams['text.color'] = COLOR_PRIMARY
rcParams['figure.dpi'] = 100
rcParams['savefig.dpi'] = 100
rcParams['savefig.bbox'] = 'tight'
rcParams['figure.constrained_layout.use'] = True

def find_project_root() -> Path:
    """Locate repository root regardless of Quarto's working directory."""
    start = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()
    for candidate in (start,) + tuple(start.parents):
        if (candidate / "data" / "analysis").exists():
            return candidate
    raise FileNotFoundError("Could not locate project root containing data/analysis directory.")

BASE = find_project_root()

def load_csv(path: str) -> pd.DataFrame:
    fp = BASE / path
    if not fp.exists():
        raise FileNotFoundError(f"Expected file not found: {fp}")
    return pd.read_csv(fp)

datetime_overview_json = BASE / "data/analysis/datetime_overview.json"
if not datetime_overview_json.exists():
    raise FileNotFoundError("datetime_overview.json missing. Run scripts/03_generate_summary.py.")
datetime_overview = json.loads(datetime_overview_json.read_text())

cal_state_by_user = load_csv("data/analysis/cal_state_by_user.csv")
message_stats = load_csv("data/analysis/message_stats.csv")
sentiment_user = load_csv("data/analysis/sentiment_user.csv")
sentiment_ai = load_csv("data/analysis/sentiment_ai.csv")
sentiment_overview = load_csv("data/analysis/sentiment_overview.csv")
topic_keywords = load_csv("data/analysis/topic_keywords.csv")

# Add intent_label column if missing (for backward compatibility)
if 'intent_label' not in topic_keywords.columns:
    # Try to load from topic_examples files as fallback
    try:
        topic_examples_user = load_csv("data/analysis/topic_examples_user.csv")
        topic_examples_assistant = load_csv("data/analysis/topic_examples_assistant.csv")

        # Create a mapping from keywords to intent_label by matching keywords exactly
        label_map = {}
        for df, role in [(topic_examples_user, 'user'), (topic_examples_assistant, 'assistant')]:
            if 'intent_label' in df.columns and 'keywords' in df.columns:
                for idx, row in df.iterrows():
                    # Use keywords as key along with role
                    label_map[(role, str(row['keywords']))] = row['intent_label']

        # Apply the mapping by matching keywords
        def get_intent_label(row):
            key = (row['message_role'], str(row['keywords']))
            return label_map.get(key, f"Topic {row.get('topic_id', '?')}")

        topic_keywords['intent_label'] = topic_keywords.apply(get_intent_label, axis=1)
    except Exception as e:
        # If fallback fails, just use topic_id as label
        topic_keywords['intent_label'] = topic_keywords.apply(
            lambda row: f"Topic {row.get('topic_id', '?')}", axis=1
        )

messages = load_csv("data/cleaned/messages_translated.csv")
messages["datetime"] = pd.to_datetime(messages["datetime"], errors="coerce")
messages = messages.dropna(subset=["datetime"])

messages["date"] = messages["datetime"].dt.date
messages["hour"] = messages["datetime"].dt.hour

user_msgs = messages[["datetime", "date", "hour", "whatsapp_id", "user_msg_en", "cal_state"]].rename(columns={"user_msg_en": "message"})
assistant_msgs = messages[["datetime", "date", "hour", "whatsapp_id", "ai_msg_en"]].rename(columns={"ai_msg_en": "message"})

message_topics_user = load_csv("data/analysis/message_topics_user.csv")
message_topics_assistant = load_csv("data/analysis/message_topics_assistant.csv")

timeline_daily = (
    user_msgs.groupby("date").size().reset_index(name="messages")
    if not user_msgs.empty else pd.DataFrame(columns=["date", "messages"])
)

timeline_hourly = (
    user_msgs.groupby("hour").size().reset_index(name="messages")
    if not user_msgs.empty else pd.DataFrame(columns=["hour", "messages"])
)

cal_state_counts = (
    cal_state_by_user.groupby("cal_state")["message_count"].sum().reset_index().sort_values("message_count", ascending=False)
    if not cal_state_by_user.empty else pd.DataFrame(columns=["cal_state", "message_count"])
)

def formatted_summary():
    total_msgs = datetime_overview.get("total_messages")
    users = datetime_overview.get("unique_users")
    start = datetime_overview.get("start_datetime")
    end = datetime_overview.get("end_datetime")
    coverage = datetime_overview.get("total_days")
    lines = [
        f"Total messages analysed: {total_msgs:,}" if total_msgs is not None else None,
        f"Unique WhatsApp IDs: {users:,}" if users is not None else None,
        f"Data range: {start} to {end}" if start and end else None,
        f"Coverage: {coverage} days" if coverage is not None else None,
    ]
    return "\n".join(line for line in lines if line)

overview_sentence = formatted_summary()

def sentiment_summary(df: pd.DataFrame, role: str) -> str:
    if df.empty:
        return f"No sentiment records for {role}."
    counts = df["sentiment_label"].value_counts(normalize=True).head(3)
    parts = [f"{label} ({pct*100:.1f}%)" for label, pct in counts.items()]
    return f"{role.capitalize()} messages most often express: " + ", ".join(parts)

sentiment_user_sentence = sentiment_summary(sentiment_user, "user")
sentiment_ai_sentence = sentiment_summary(sentiment_ai, "assistant")

def topic_summary(df: pd.DataFrame, role: str, top_n: int = 5) -> str:
    role_df = df[df["message_role"] == role]
    if role_df.empty:
        return f"No topics were identified for {role}."
    top_rows = role_df.sort_values("message_count", ascending=False).head(top_n)
    fragments = [
        f"Topic {row.topic_id}: {row.keywords} ({row.message_count} msgs)"
        for row in top_rows.itertuples()
    ]
    return "\n".join(fragments)

user_topic_sentence = topic_summary(topic_keywords, "user")
assistant_topic_sentence = topic_summary(topic_keywords, "assistant")

def cal_state_table():
    if cal_state_counts.empty:
        return pd.DataFrame(columns=["cal_state", "message_count"])
    cal_state_counts["share_pct"] = cal_state_counts["message_count"] / cal_state_counts["message_count"].sum() * 100
    return cal_state_counts


def _keyword_variants(keywords: str) -> list[str]:
    """Extract keyword variants including underscore replacements and common inflections."""
    tokens = [token.strip() for token in (keywords or "").split(",") if token.strip()]
    variants = set()
    for token in tokens:
        base = token.lower()
        variants.add(base)
        variants.add(base.replace("_", " "))

        # Add common inflections to catch lemmatization mismatches
        if not base.endswith(('s', 'ed', 'ing', 'tion', 'ment')):
            variants.add(base + "s")
            variants.add(base + "ed")
            variants.add(base + "ing")

        # Remove trailing 'e' before adding -ed/-ing
        if base.endswith('e') and len(base) > 2:
            stem = base[:-1]
            variants.add(stem + "ed")
            variants.add(stem + "ing")

        # Handle -ion/-ment variations
        if base.endswith('e'):
            variants.add(base[:-1] + "ion")
        if not base.endswith('ment'):
            variants.add(base + "ment")

    return sorted([variant for variant in variants if variant], key=len, reverse=True)


def highlight_keywords(text: str, keywords: str) -> str:
    if not isinstance(text, str) or not text.strip():
        return ""
    highlighted = text
    for term in _keyword_variants(keywords):
        # Use word boundaries to match whole words, handle lemmatization variants
        pattern = re.compile(rf"(?i)\b({re.escape(term)})\b")
        highlighted = pattern.sub(r"<mark>\1</mark>", highlighted)
    return highlighted


def sample_topic_examples(assign_df: pd.DataFrame, top_topics: pd.DataFrame, per_topic: int = 3) -> list[dict]:
    examples: list[dict] = []
    for _, topic_row in top_topics.iterrows():
        topic_id = int(topic_row.get("topic_id", topic_row.get("Topic", -1)))
        intent_label = str(topic_row.get("intent_label", f"Topic {topic_id}"))
        keywords = str(topic_row.get("keywords", topic_row.get("Name", "")))
        subset = assign_df[assign_df["topic_id"] == topic_id]
        if "original_message" in subset:
            sample_msgs = subset["original_message"].dropna().astype(str).head(per_topic).tolist()
        else:
            sample_msgs = []
        highlighted = [highlight_keywords(msg, keywords) for msg in sample_msgs if msg]
        examples.append({
            "topic_id": topic_id,
            "intent_label": intent_label,
            "keywords": keywords,
            "examples": highlighted,
        })
    return examples

``` 

# Overview

This report analyzes student-AI interactions from the Udhyam learning platform. The analysis examines message patterns, sentiment, topics, and conversation dynamics to understand how students engage with the AI assistant.

```{python}
#| output: asis
print(f"{overview_sentence}")
```

## Key Findings

```{python}
#| output: asis
# Create full sentence summaries for key findings
findings = []

# Sentiment patterns
user_sentiment_text = sentiment_user_sentence.replace("User messages most often express: ", "").lower() if sentiment_user_sentence else ""
ai_sentiment_text = sentiment_ai_sentence.replace("Assistant messages most often express: ", "").lower() if sentiment_ai_sentence else ""

if user_sentiment_text:
    findings.append(f"User messages predominantly express {user_sentiment_text}, indicating their overall emotional tone during interactions with the AI assistant.")

if ai_sentiment_text:
    findings.append(f"The AI assistant maintains a tone that is primarily {ai_sentiment_text}, reflecting consistent pedagogical support in its responses.")

# Topic findings
if user_topic_sentence:
    findings.append(f"Students engage most frequently with topics related to: {user_topic_sentence.split(chr(10))[0].replace('Topic ', '').lower()}. These represent the primary areas where students seek AI assistance and guidance.")

if assistant_topic_sentence:
    findings.append(f"The AI assistant's responses are concentrated in areas including {assistant_topic_sentence.split(chr(10))[0].replace('Topic ', '').lower()}, showing focused coverage of student needs.")

for finding in findings:
    if finding:
        print(f"{finding}\n\n")
```

---

# Part 1: Message Activity {.tabset}

## 1.1 Daily Volume

```{python}
#| label: fig-daily-volume
#| fig-cap: "Daily user message volume over time"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not timeline_daily.empty:
    sns.lineplot(data=timeline_daily, x="date", y="messages", marker="o",
                 color=COLOR_USER, linewidth=2.5, markersize=8, ax=ax)
    ax.set_title("Daily User Message Volume", fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Date", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_ylim(bottom=0)
    plt.xticks(rotation=45, ha="right")
    ax.grid(True, alpha=0.3)
else:
    ax.text(0.5, 0.5, "No user message activity records",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

Daily message volume shows the temporal distribution of student engagement with the AI assistant.

## 1.2 Hourly Distribution

```{python}
#| label: fig-hourly-distribution
#| fig-cap: "Hourly distribution of user messages throughout the day"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not timeline_hourly.empty:
    sns.barplot(data=timeline_hourly, x="hour", y="messages",
                color=COLOR_USER, alpha=0.8, ax=ax)
    ax.set_title("Hourly Distribution of User Messages",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Hour of Day", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels on bars
    for container in ax.containers:
        ax.bar_label(container, fontsize=9)
else:
    ax.text(0.5, 0.5, "No hourly data available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

This distribution reveals when students are most active in using the platform throughout the day.

## 1.3 CAL State Distribution

```{python}
#| label: tbl-cal-state
#| tbl-cap: "State distribution with message counts and percentages"
from IPython.display import display, Markdown

cal_table = cal_state_table()
if not cal_table.empty:
    # Format the table for better display
    cal_display = cal_table.copy()
    cal_display.columns = ['CAL State', 'Message Count', 'Share (%)']
    cal_display['Share (%)'] = cal_display['Share (%)'].round(1)
    display(Markdown(cal_display.to_markdown(index=False)))
else:
    print("No CAL state data available")
```

The table above shows which states generate the most engagement, indicating where students spend the most time and where the AI is most frequently engaged.

## 1.4 Message Statistics

```{python}
#| label: tbl-message-stats
#| tbl-cap: "Summary statistics for message activity"
from IPython.display import display, Markdown

if not message_stats.empty:
    display(Markdown(message_stats.to_markdown(index=False)))
else:
    print("No message statistics available")
```

---

# Part 3: Sentiment Analysis {.tabset}

Sentiment analysis examines the emotional tone of messages exchanged between students and the AI assistant. Understanding sentiment helps assess user satisfaction and the quality of AI responses.

## Understanding Sentiment Analysis

**What is being measured?**

Messages are analyzed for emotional tone and classified into sentiment categories (typically Positive, Neutral, and Negative). This helps us understand:

- **User sentiment**: How students feel when interacting with the AI
- **AI sentiment**: Whether the AI maintains an appropriate, supportive tone
- **Interaction quality**: The overall emotional dynamic of conversations

**Technical methodology:**

The sentiment analysis is performed using transformer-based natural language processing models. Each message is processed through the following pipeline:

1. **Text preprocessing**: Messages are cleaned and normalized, preserving emotionally significant elements like punctuation and capitalization
2. **Model inference**: A pre-trained sentiment classification model (typically BERT-based or similar transformer architecture) analyzes the text and assigns probability scores to each sentiment category
3. **Classification**: Messages are labeled as Positive, Neutral, or Negative based on the highest probability score
4. **Aggregation**: Results are grouped by message role (user vs. assistant) to compare sentiment patterns

The model has been fine-tuned on conversational text data to better capture the nuances of educational interactions, including informal language, code snippets, and domain-specific terminology common in learning platforms.

## 3.1 Sentiment Comparison

```{python}
#| label: fig-sentiment-comparison
#| fig-cap: "Sentiment distribution comparison between user and AI messages"

# Create sentiment_counts for visualization
sentiment_long = pd.concat([
    sentiment_user.assign(role="user"),
    sentiment_ai.assign(role="assistant"),
], ignore_index=True, sort=False)

sentiment_counts = (
    sentiment_long.groupby(["role", "sentiment_label"]).size().reset_index(name="messages")
)

fig, ax = plt.subplots(figsize=(8, 4.5))
if not sentiment_counts.empty:
    # Map sentiment labels to colors
    sentiment_colors = {
        'Positive': COLOR_POSITIVE,
        'Neutral': COLOR_NEUTRAL,
        'Negative': COLOR_NEGATIVE,
        'positive': COLOR_POSITIVE,
        'neutral': COLOR_NEUTRAL,
        'negative': COLOR_NEGATIVE
    }

    # Prepare data for plotting
    plot_data = sentiment_counts.copy()
    plot_data['role'] = plot_data['role'].str.capitalize()

    # Create grouped bar chart
    x = np.arange(len(plot_data['sentiment_label'].unique()))
    width = 0.35
    user_data = plot_data[plot_data['role'] == 'User']
    ai_data = plot_data[plot_data['role'] == 'Assistant']

    ax.bar(x - width/2, user_data['messages'], width, label='User',
           color=COLOR_USER, alpha=0.8)
    ax.bar(x + width/2, ai_data['messages'], width, label='Assistant',
           color=COLOR_AI, alpha=0.8)

    ax.set_title("Sentiment Distribution by Role",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Sentiment", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_xticks(x)
    ax.set_xticklabels(user_data['sentiment_label'].str.capitalize())
    ax.legend(title='Role', fontsize=10, title_fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels
    for container in ax.containers:
        ax.bar_label(container, fontsize=9)
else:
    ax.text(0.5, 0.5, "No sentiment records available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

This comparison reveals whether users and the AI assistant maintain different emotional tones during interactions.

## 3.2 Overall Sentiment Summary

```{python}
#| label: tbl-sentiment-overview
#| tbl-cap: "Overall sentiment statistics"
from IPython.display import display, Markdown

if not sentiment_overview.empty:
    display(Markdown(sentiment_overview.to_markdown(index=False)))
else:
    print("No sentiment overview data available")
```

---

# Part 3: Topic Modeling {.tabset}

Topic modeling identifies the main themes and subjects discussed in conversations. This analysis reveals what students talk about most frequently and how the AI responds. 

## Understanding Topic Modeling

**What is topic modeling?**

Topic modeling uses natural language processing to automatically discover themes in text data. Each topic is represented by:

- **Keywords**: The most characteristic words for that topic
- **Message count**: How many messages belong to that topic
- **Role**: Whether the topic appears in user or AI messages

This helps identify common student questions, concerns, and the AI's response patterns.

**Technical methodology:**

The topic modeling is performed using BERTopic, a modern topic modeling technique that leverages transformer embeddings and clustering algorithms:

1. **Text embedding**: Each message is converted into a high-dimensional semantic vector using sentence transformers (SBERT). These embeddings capture the contextual meaning of the text, not just keyword frequency
2. **Dimensionality reduction**: UMAP (Uniform Manifold Approximation and Projection) reduces the embedding dimensions while preserving semantic relationships, making clusters more identifiable
3. **Clustering**: HDBSCAN (Hierarchical Density-Based Spatial Clustering) groups semantically similar messages together based on their embeddings. Unlike traditional methods like LDA, this approach doesn't require pre-specifying the number of topics
4. **Topic representation**: For each cluster, c-TF-IDF (class-based Term Frequency-Inverse Document Frequency) extracts the most representative keywords that distinguish one topic from others
5. **OpenAI refinement (optional)**: When enabled, raw keywords and sample messages from each topic are sent to OpenAI's GPT models (e.g., gpt-4o-mini) to generate human-readable topic labels. This produces more natural, interpretable descriptions compared to keyword lists alone
6. **Role separation**: Topics are analyzed separately for user messages and AI messages to understand the conversation dynamics from both perspectives

This approach provides more coherent and interpretable topics compared to traditional methods, especially for short conversational messages where context is crucial.

**How to interpret OpenAI-refined topics:**

When OpenAI representation is enabled (using the `--use-openai-representation` flag), the topic labels you see are **not** raw statistical keywords. Instead, they are natural language interpretations generated by a large language model (GPT) based on:

- The initial keyword list from c-TF-IDF
- Sample messages from the topic cluster
- Context about the domain and conversation type

**What this means for interpretation:**

- **More readable**: Topics appear as coherent phrases rather than disconnected keywords (e.g., "Business model validation strategies" instead of "business, model, validate, customer, market")
- **Higher-level abstraction**: The LLM synthesizes the underlying theme rather than just listing common terms
- **Consistency**: Using temperature=0.0 ensures deterministic, consistent labeling across runs
- **Trade-offs**: While more interpretable, OpenAI labels may occasionally oversimplify or miss nuances present in the raw keywords. For critical analysis, review both the OpenAI label and the underlying keyword distribution

**When OpenAI refinement is NOT used** (default behavior), you'll see raw keyword lists extracted via c-TF-IDF, which represent the most statistically distinctive terms for each topic. These are more granular but require more manual interpretation to understand the broader theme.

## 3.1 User Message Topics

::: {.figure-scroll}
```{python}
#| label: fig-user-topics
#| fig-cap: "Top 15 topics in user messages"
fig, ax = plt.subplots(figsize=(12, 10))
if not topic_keywords.empty:
    # Get top 15 topics for user messages
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False).head(15)

    if not user_topics.empty:
        # Sort by message count for better visualization
        user_topics = user_topics.sort_values('message_count', ascending=True)

        # Create horizontal bar chart
        y_pos = np.arange(len(user_topics))
        bars = ax.barh(y_pos, user_topics['message_count'], color=COLOR_USER, alpha=0.8)
        ax.set_yticks(y_pos)
        # Use intent_label if available, otherwise fall back to keywords
        labels = user_topics['intent_label'] if 'intent_label' in user_topics.columns else user_topics['keywords']
        ax.set_yticklabels(labels, fontsize=9)
        ax.set_title("Top Topics in User Messages",
                     fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
        ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
        ax.set_ylabel("Topic", fontweight='bold', fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (idx, row) in enumerate(user_topics.iterrows()):
            ax.text(row['message_count'] + max(user_topics['message_count']) * 0.01,
                    i, f"{row['message_count']:,}",
                    va='center', fontsize=8)
    else:
        ax.text(0.5, 0.5, "No user topics identified",
                ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
        ax.set_xticks([])
        ax.set_yticks([])
else:
    ax.text(0.5, 0.5, "Topic modeling did not return any clusters",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```
:::

The user topic visualization reveals what students are asking about and discussing most frequently. These topics represent the primary concerns, questions, and areas where students seek assistance from the AI.

## 3.2 AI Message Topics

::: {.figure-scroll}
```{python}
#| label: fig-ai-topics
#| fig-cap: "Top 15 topics in AI messages"
fig, ax = plt.subplots(figsize=(12, 10))
if not topic_keywords.empty:
    # Get top 15 topics for AI messages
    ai_topics = topic_keywords[topic_keywords['message_role'] == 'assistant'].sort_values('message_count', ascending=False).head(15)

    if not ai_topics.empty:
        # Sort by message count for better visualization
        ai_topics = ai_topics.sort_values('message_count', ascending=True)

        # Create horizontal bar chart
        y_pos = np.arange(len(ai_topics))
        bars = ax.barh(y_pos, ai_topics['message_count'], color=COLOR_AI, alpha=0.8)
        ax.set_yticks(y_pos)
        # Use intent_label if available, otherwise fall back to keywords
        labels = ai_topics['intent_label'] if 'intent_label' in ai_topics.columns else ai_topics['keywords']
        ax.set_yticklabels(labels, fontsize=9)
        ax.set_title("Top Topics in AI Messages",
                     fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
        ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
        ax.set_ylabel("Topic", fontweight='bold', fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (idx, row) in enumerate(ai_topics.iterrows()):
            ax.text(row['message_count'] + max(ai_topics['message_count']) * 0.01,
                    i, f"{row['message_count']:,}",
                    va='center', fontsize=8)
    else:
        ax.text(0.5, 0.5, "No AI topics identified",
                ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
        ax.set_xticks([])
        ax.set_yticks([])
else:
    ax.text(0.5, 0.5, "Topic modeling did not return any clusters",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```
:::

The AI topic visualization shows the main themes in the assistant's responses. These topics reflect how the AI is addressing student needs and what types of support it provides most frequently.

## 3.3 Interpreting Topic Patterns

To ground the topic labels, the tables below show real message excerpts with topic keywords highlighted. This makes interpretations like “Casual affirmations” tangible by displaying the language patterns that define each cluster.

Notes:

- Highlights are derived from each topic’s keyword list. Because messages are lemmatized during preprocessing, highlighted forms may include closely related surface variants.
- Examples come directly from the translated messages (the English text), giving a clear view of the student or AI phrasing that BERTopic grouped together.

### User topic examples

```{python}
#| output: asis

def _escape_markdown(text: str) -> str:
    """Escape characters that break markdown tables."""
    if not isinstance(text, str):
        text = str(text)
    return text.replace("|", "\\|").replace("\n", " ")

def _render_topic_table(rows) -> str:
    header = [
        '::: {.tbl-colwidths="[0.15,0.20,0.65]"}',
        '| Topic | Keywords | Example messages |',
        '|:------|:---------|:------------------|',
    ]
    body = []
    for row in rows:
        topic_display = f"**{_escape_markdown(row['topic_id'])}: {_escape_markdown(row['intent_label'])}**"
        keywords_display = f"*{_escape_markdown(row['keywords'])}*"
        example_lines = "<br>".join(f"&bull; {_escape_markdown(example)}" for example in row['examples'])
        body.append(f"| {topic_display} | {keywords_display} | {example_lines} |")
    footer = [':::']
    return "\n".join(header + body + footer)

if not topic_keywords.empty and not message_topics_user.empty:
    top_user = (
        topic_keywords[topic_keywords['message_role'] == 'user']
        .sort_values('message_count', ascending=False)
        .head(15)
    )
    rows = sample_topic_examples(message_topics_user, top_user, per_topic=3)
    table_md = _render_topic_table(rows)
    output = [
        "<details>",
        "<summary>Show user topic examples</summary>",
        "",
        table_md,
        "",
        "</details>",
    ]
    print("\n".join(output))
else:
    print("No user topic examples available.")
```

### AI topic examples

```{python}
#| output: asis

if not topic_keywords.empty and not message_topics_assistant.empty:
    top_ai = (
        topic_keywords[topic_keywords['message_role'] == 'assistant']
        .sort_values('message_count', ascending=False)
        .head(15)
    )
    rows = sample_topic_examples(message_topics_assistant, top_ai, per_topic=3)
    table_md = _render_topic_table(rows)
    output = [
        "<details>",
        "<summary>Show AI topic examples</summary>",
        "",
        table_md,
        "",
        "</details>",
    ]
    print("\n".join(output))
else:
    print("No AI topic examples available.")
```

**Topic Alignment and Pedagogical Insights:**

The relationship between user and AI topics reveals the quality of conversational alignment:

- **Responsive teaching**: When AI topics mirror user topics with added depth, it indicates the assistant is directly addressing student needs
- **Proactive guidance**: Topics in AI messages that extend beyond immediate user questions suggest the assistant is providing broader context and anticipatory support
- **Knowledge gaps**: Frequent topics about basic concepts may indicate areas where students need more foundational instruction or clearer explanations
- **Engagement patterns**: The diversity and specificity of topics reflect how deeply students are engaging with course material

By analyzing these patterns over time, educators can identify which topics require enhanced content, where students struggle most, and how to optimize the AI's pedagogical approach.

---

# Summary and Recommendations

This analysis provides comprehensive insights into how students interact with the AI assistant on the Udhyam learning platform. By examining message patterns, sentiment, topics, and engagement dynamics, we have identified key trends that can guide platform improvements and optimize the learning experience.

## Key Insights

```{python}
#| output: asis
# Generate full sentence insights
insights = []

# Message activity insights
if not timeline_daily.empty and not timeline_hourly.empty:
    peak_hour = timeline_hourly.loc[timeline_hourly['messages'].idxmax(), 'hour']
    total_messages = timeline_daily['messages'].sum()
    insights.append(f"Student engagement is substantial, with a total of {total_messages:,} messages exchanged over the analysis period. Peak activity occurs around hour {peak_hour}, indicating a concentrated window of student engagement that should be considered when scheduling support and platform maintenance.")

# CAL state insights
if not cal_state_counts.empty:
    top_state = cal_state_counts.iloc[0]
    insights.append(f"The '{top_state['cal_state']}' learning state is the primary focus of student interactions, generating {top_state['message_count']:,} messages. This high-volume state represents a critical area where students spend most of their time and where the AI assistant provides the most support. Optimizing content and AI responses in this state could yield significant improvements in student outcomes.")

# Sentiment insights
if not sentiment_counts.empty:
    user_sentiments = sentiment_counts[sentiment_counts['role'] == 'user']
    if not user_sentiments.empty:
        dominant_sentiment = user_sentiments.loc[user_sentiments['messages'].idxmax(), 'sentiment_label']
        ai_sentiments = sentiment_counts[sentiment_counts['role'] == 'assistant']
        dominant_ai_sentiment = ai_sentiments.loc[ai_sentiments['messages'].idxmax(), 'sentiment_label'] if not ai_sentiments.empty else "unknown"
        insights.append(f"Student sentiment is predominantly {dominant_sentiment.lower()}, which reflects their emotional state when interacting with the platform. The AI assistant maintains a {dominant_ai_sentiment.lower()} tone in its responses, demonstrating consistent emotional support. Maintaining or improving this positive interaction dynamic is essential for student satisfaction and engagement.")

# Topic insights
if not topic_keywords.empty:
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False)
    if not user_topics.empty:
        top_topic = user_topics.iloc[0]
        insights.append(f"Student discussions are concentrated around specific topics, with '{top_topic['keywords']}' being the most frequently discussed area with {top_topic['message_count']} messages. This concentration indicates clear student interests and pain points that represent opportunities for targeted content development and AI response optimization.")

for insight in insights:
    if insight:
        print(f"{insight}\n\n")
```

## Recommendations and Action Items

Based on these findings and the patterns revealed throughout this analysis, we recommend the following strategic actions:

**Optimize Peak Engagement Times.** The analysis reveals concentrated peak activity during specific hours of the day. The platform should schedule proactive AI interventions, content updates, and system maintenance during off-peak hours to avoid disrupting student engagement. Consider implementing notification strategies that leverage these peak activity windows to encourage timely student interaction with the AI.

**Prioritize High-Impact Learning States.** Since the top learning states account for the majority of student interactions, development resources should be allocated to enhance content, AI responses, and user experience in these high-volume contexts. This focused investment will directly address where students spend most of their time and where improvements will have the greatest impact.

**Maintain and Strengthen Positive Sentiment.** The predominantly positive sentiment observed in student interactions indicates a healthy engagement environment. The platform should continue monitoring sentiment trends to identify any deterioration and implement measures to sustain the current positive atmosphere. When negative sentiment is detected, root cause analysis should be conducted to address specific pain points.

**Develop Topic-Specific Content and Responses.** The concentrated distribution of student discussions around specific topics provides clear guidance for content development. Create specialized learning modules, tutorial content, and AI response templates for frequently discussed topics to provide more targeted and effective support. Use topic keywords to identify areas where additional explanatory content or clarification may be beneficial.

**Establish Continuous Monitoring Practices.** Regular re-analysis of student interaction data should become part of the platform's operational routine. Set up monthly or quarterly analysis runs using this pipeline to track changes in engagement patterns, sentiment trends, and topic distributions. This continuous monitoring will enable early detection of issues and opportunities for optimization.

**Conduct Qualitative Validation.** While quantitative analysis provides valuable insights, conducting follow-up user interviews or focus groups with students can provide deeper context about why certain topics are frequently discussed and what aspects of the learning experience are most valuable. This qualitative feedback should inform iterative improvements to the platform and AI responses.

## Next Steps

To implement these recommendations and continue improving the student-AI interaction experience, the following steps should be taken in priority order. First, establish a regular analysis schedule to generate updated reports as new data becomes available, ensuring that decision-making is always based on current patterns. Second, conduct a deep dive analysis of the highest-volume learning states to identify specific improvements that can be quickly implemented. Third, work with content developers to create specialized resources for the most frequently discussed topics. Finally, begin qualitative research through user interviews to validate the quantitative findings and gather contextual insights that numbers alone cannot provide.

---

```{python}
#| output: asis
print(f"**Report generated:** {pd.Timestamp.today().strftime('%B %d, %Y')}")
print(f"\n**Analysis period:** {datetime_overview.get('start_datetime', 'N/A')} to {datetime_overview.get('end_datetime', 'N/A')}")
print(f"\n**Total messages analyzed:** {datetime_overview.get('total_messages', 'N/A'):,}")
```
