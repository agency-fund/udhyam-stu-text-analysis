---
title: "Udhyam Student Text Analysis Report"
author: "Udhyam Analytics"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    embed-resources: true
    fig-width: 8
    fig-height: 5
    page-layout: article
execute:
  warning: false
  message: false
  echo: false
  fig.align: center
jupyter: py312_arm_clean
---

```{=html}
<style>
.quarto-figure {
  max-width: 100%;
  overflow-x: hidden;
}
img, .quarto-figure img {
  max-width: 100% !important;
  height: auto !important;
  display: block;
}
.figure-scroll {
  overflow-x: auto;
}
.figure-scroll .quarto-figure {
  max-width: none;
  overflow-x: visible;
  display: inline-block;
}
.figure-scroll img {
  max-width: none !important;
}
</style>
```

```{python}
#| label: setup
import json
import re
from pathlib import Path
from textwrap import fill
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import rcParams

# Set professional theme
sns.set_style("whitegrid")
plt.style.use("seaborn-v0_8-darkgrid")

# Define color palette
COLOR_USER = "#3498DB"
COLOR_AI = "#E67E22"
COLOR_POSITIVE = "#2ECC71"
COLOR_NEUTRAL = "#95A5A6"
COLOR_NEGATIVE = "#E74C3C"
COLOR_PRIMARY = "#2C3E50"

# Configure matplotlib defaults
rcParams['figure.facecolor'] = 'white'
rcParams['axes.facecolor'] = 'white'
rcParams['axes.edgecolor'] = '#E5E7EB'
rcParams['axes.labelcolor'] = COLOR_PRIMARY
rcParams['axes.titlesize'] = 14
rcParams['axes.titleweight'] = 'bold'
rcParams['axes.labelsize'] = 10
rcParams['axes.labelweight'] = 'bold'
rcParams['xtick.color'] = '#34495E'
rcParams['ytick.color'] = '#34495E'
rcParams['xtick.labelsize'] = 9
rcParams['ytick.labelsize'] = 9
rcParams['grid.color'] = '#E5E7EB'
rcParams['grid.alpha'] = 0.6
rcParams['text.color'] = COLOR_PRIMARY
rcParams['figure.dpi'] = 100
rcParams['savefig.dpi'] = 100
rcParams['savefig.bbox'] = 'tight'
rcParams['figure.constrained_layout.use'] = True

def find_project_root() -> Path:
    """Locate repository root regardless of Quarto's working directory."""
    start = Path(__file__).resolve().parent if "__file__" in globals() else Path.cwd()
    for candidate in (start,) + tuple(start.parents):
        if (candidate / "data" / "analysis").exists():
            return candidate
    raise FileNotFoundError("Could not locate project root containing data/analysis directory.")

BASE = find_project_root()

def load_csv(path: str) -> pd.DataFrame:
    fp = BASE / path
    if not fp.exists():
        raise FileNotFoundError(f"Expected file not found: {fp}")
    return pd.read_csv(fp)

datetime_overview_json = BASE / "data/analysis/datetime_overview.json"
if not datetime_overview_json.exists():
    raise FileNotFoundError("datetime_overview.json missing. Run scripts/03_generate_summary.py.")
datetime_overview = json.loads(datetime_overview_json.read_text())

cal_state_by_user = load_csv("data/analysis/cal_state_by_user.csv")
message_stats = load_csv("data/analysis/message_stats.csv")
sentiment_user = load_csv("data/analysis/sentiment_user.csv")
sentiment_ai = load_csv("data/analysis/sentiment_ai.csv")
sentiment_overview = load_csv("data/analysis/sentiment_overview.csv")
topic_keywords = load_csv("data/analysis/topic_keywords.csv")

# Add intent_label column if missing (for backward compatibility)
if 'intent_label' not in topic_keywords.columns:
    # Try to load from topic_examples files as fallback
    try:
        topic_examples_user = load_csv("data/analysis/topic_examples_user.csv")
        topic_examples_assistant = load_csv("data/analysis/topic_examples_assistant.csv")

        # Create a mapping from keywords to intent_label by matching keywords exactly
        label_map = {}
        for df, role in [(topic_examples_user, 'user'), (topic_examples_assistant, 'assistant')]:
            if 'intent_label' in df.columns and 'keywords' in df.columns:
                for idx, row in df.iterrows():
                    # Use keywords as key along with role
                    label_map[(role, str(row['keywords']))] = row['intent_label']

        # Apply the mapping by matching keywords
        def get_intent_label(row):
            key = (row['message_role'], str(row['keywords']))
            return label_map.get(key, f"Topic {row.get('topic_id', '?')}")

        topic_keywords['intent_label'] = topic_keywords.apply(get_intent_label, axis=1)
    except Exception as e:
        # If fallback fails, just use topic_id as label
        topic_keywords['intent_label'] = topic_keywords.apply(
            lambda row: f"Topic {row.get('topic_id', '?')}", axis=1
        )

messages = load_csv("data/cleaned/messages_translated.csv")
messages["datetime"] = pd.to_datetime(messages["datetime"], errors="coerce")
messages = messages.dropna(subset=["datetime"])

messages["date"] = messages["datetime"].dt.date
messages["hour"] = messages["datetime"].dt.hour

user_msgs = messages[["datetime", "date", "hour", "whatsapp_id", "user_msg_en", "cal_state"]].rename(columns={"user_msg_en": "message"})
assistant_msgs = messages[["datetime", "date", "hour", "whatsapp_id", "ai_msg_en"]].rename(columns={"ai_msg_en": "message"})

message_topics_user = load_csv("data/analysis/message_topics_user.csv")
message_topics_assistant = load_csv("data/analysis/message_topics_assistant.csv")

timeline_daily = (
    user_msgs.groupby("date").size().reset_index(name="messages")
    if not user_msgs.empty else pd.DataFrame(columns=["date", "messages"])
)

timeline_hourly = (
    user_msgs.groupby("hour").size().reset_index(name="messages")
    if not user_msgs.empty else pd.DataFrame(columns=["hour", "messages"])
)

cal_state_counts = (
    cal_state_by_user.groupby("cal_state")["message_count"].sum().reset_index().sort_values("message_count", ascending=False)
    if not cal_state_by_user.empty else pd.DataFrame(columns=["cal_state", "message_count"])
)

def formatted_summary():
    total_msgs = datetime_overview.get("total_messages")
    users = datetime_overview.get("unique_users")
    start = datetime_overview.get("start_datetime")
    end = datetime_overview.get("end_datetime")
    coverage = datetime_overview.get("total_days")
    lines = [
        f"Total messages analysed: {total_msgs:,}" if total_msgs is not None else None,
        f"Unique WhatsApp IDs: {users:,}" if users is not None else None,
        f"Data range: {start} to {end}" if start and end else None,
        f"Coverage: {coverage} days" if coverage is not None else None,
    ]
    return "\n".join(line for line in lines if line)

overview_sentence = formatted_summary()

def sentiment_summary(df: pd.DataFrame, role: str) -> str:
    if df.empty:
        return f"No sentiment records for {role}."
    counts = df["sentiment_label"].value_counts(normalize=True).head(3)
    parts = [f"{label} ({pct*100:.1f}%)" for label, pct in counts.items()]
    return f"{role.capitalize()} messages most often express: " + ", ".join(parts)

sentiment_user_sentence = sentiment_summary(sentiment_user, "user")
sentiment_ai_sentence = sentiment_summary(sentiment_ai, "assistant")

def topic_summary(df: pd.DataFrame, role: str, top_n: int = 5) -> str:
    role_df = df[df["message_role"] == role]
    if role_df.empty:
        return f"No topics were identified for {role}."
    top_rows = role_df.sort_values("message_count", ascending=False).head(top_n)
    fragments = [
        f"Topic {row.topic_id}: {row.keywords} ({row.message_count} msgs)"
        for row in top_rows.itertuples()
    ]
    return "\n".join(fragments)

user_topic_sentence = topic_summary(topic_keywords, "user")
assistant_topic_sentence = topic_summary(topic_keywords, "assistant")

def cal_state_table():
    if cal_state_counts.empty:
        return pd.DataFrame(columns=["cal_state", "message_count"])
    cal_state_counts["share_pct"] = cal_state_counts["message_count"] / cal_state_counts["message_count"].sum() * 100
    return cal_state_counts


def _keyword_variants(keywords: str) -> list[str]:
    """Extract keyword variants including underscore replacements and common inflections."""
    tokens = [token.strip() for token in (keywords or "").split(",") if token.strip()]
    variants = set()
    for token in tokens:
        base = token.lower()
        variants.add(base)
        variants.add(base.replace("_", " "))

        # Add common inflections to catch lemmatization mismatches
        if not base.endswith(('s', 'ed', 'ing', 'tion', 'ment')):
            variants.add(base + "s")
            variants.add(base + "ed")
            variants.add(base + "ing")

        # Remove trailing 'e' before adding -ed/-ing
        if base.endswith('e') and len(base) > 2:
            stem = base[:-1]
            variants.add(stem + "ed")
            variants.add(stem + "ing")

        # Handle -ion/-ment variations
        if base.endswith('e'):
            variants.add(base[:-1] + "ion")
        if not base.endswith('ment'):
            variants.add(base + "ment")

    return sorted([variant for variant in variants if variant], key=len, reverse=True)


def highlight_keywords(text: str, keywords: str) -> str:
    if not isinstance(text, str) or not text.strip():
        return ""
    highlighted = text
    for term in _keyword_variants(keywords):
        # Use word boundaries to match whole words, handle lemmatization variants
        pattern = re.compile(rf"(?i)\b({re.escape(term)})\b")
        highlighted = pattern.sub(r"<mark>\1</mark>", highlighted)
    return highlighted


def sample_topic_examples(assign_df: pd.DataFrame, top_topics: pd.DataFrame, per_topic: int = 3) -> list[dict]:
    examples: list[dict] = []
    for _, topic_row in top_topics.iterrows():
        topic_id = int(topic_row.get("topic_id", topic_row.get("Topic", -1)))
        intent_label = str(topic_row.get("intent_label", f"Topic {topic_id}"))
        keywords = str(topic_row.get("keywords", topic_row.get("Name", "")))
        subset = assign_df[assign_df["topic_id"] == topic_id]
        if "original_message" in subset:
            sample_msgs = subset["original_message"].dropna().astype(str).head(per_topic).tolist()
        else:
            sample_msgs = []
        highlighted = [highlight_keywords(msg, keywords) for msg in sample_msgs if msg]
        examples.append({
            "topic_id": topic_id,
            "intent_label": intent_label,
            "keywords": keywords,
            "examples": highlighted,
        })
    return examples

``` 

# Overview

This report analyzes student-AI interactions from the Udhyam learning platform. The analysis examines message patterns, sentiment, topics, and conversation dynamics to understand how students engage with the AI assistant. 

```{python}
#| output: asis
print(f"**Data Scope:** {overview_sentence}")
```

## Key Findings

```{python}
#| output: asis
findings = [
    f"**Sentiment Patterns (User):** {sentiment_user_sentence}",
    f"**Sentiment Patterns (Assistant):** {sentiment_ai_sentence}",
    f"**Key User Topics:**\n\n{user_topic_sentence}",
    f"**Assistant Response Themes:**\n\n{assistant_topic_sentence}",
]
for finding in findings:
    if finding:
        print(f"{finding}\n\n")
```

---

# Part 1: Message Activity {.tabset}

## 1.1 Daily Volume

```{python}
#| label: fig-daily-volume
#| fig-cap: "Daily user message volume over time"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not timeline_daily.empty:
    sns.lineplot(data=timeline_daily, x="date", y="messages", marker="o",
                 color=COLOR_USER, linewidth=2.5, markersize=8, ax=ax)
    ax.set_title("Daily User Message Volume", fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Date", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_ylim(bottom=0)
    plt.xticks(rotation=45, ha="right")
    ax.grid(True, alpha=0.3)
else:
    ax.text(0.5, 0.5, "No user message activity records",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

Daily message volume shows the temporal distribution of student engagement with the AI assistant.

## 1.2 Hourly Distribution

```{python}
#| label: fig-hourly-distribution
#| fig-cap: "Hourly distribution of user messages throughout the day"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not timeline_hourly.empty:
    sns.barplot(data=timeline_hourly, x="hour", y="messages",
                color=COLOR_USER, alpha=0.8, ax=ax)
    ax.set_title("Hourly Distribution of User Messages",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Hour of Day", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels on bars
    for container in ax.containers:
        ax.bar_label(container, fontsize=9)
else:
    ax.text(0.5, 0.5, "No hourly data available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

This distribution reveals when students are most active in using the platform throughout the day.

## 1.3 Message Statistics

```{python}
#| label: tbl-message-stats
#| tbl-cap: "Summary statistics for message activity"
from IPython.display import display, Markdown

if not message_stats.empty:
    display(Markdown(message_stats.to_markdown(index=False)))
else:
    print("No message statistics available")
```

---

# Part 2: CAL State Engagement {.tabset}

CAL (Conversational AI Learning) States represent different stages or contexts of student interactions with the AI assistant. This section analyzes which states generate the most engagement.

## 2.1 State Distribution Table

```{python}
#| label: tbl-cal-state
#| tbl-cap: "CAL State distribution with message counts and percentages"
from IPython.display import display, Markdown

cal_table = cal_state_table()
if not cal_table.empty:
    # Format the table for better display
    cal_display = cal_table.copy()
    cal_display.columns = ['CAL State', 'Message Count', 'Share (%)']
    cal_display['Share (%)'] = cal_display['Share (%)'].round(1)
    display(Markdown(cal_display.to_markdown(index=False)))
else:
    print("No CAL state data available")
```

## 2.2 Top CAL States Visualization

```{python}
#| label: fig-cal-states
#| fig-cap: "Top 10 CAL states by message volume"
fig, ax = plt.subplots(figsize=(8, 5.5))
if not cal_state_counts.empty:
    top_states = cal_state_counts.head(10)
    bars = ax.barh(range(len(top_states)), top_states['message_count'],
                   color=COLOR_USER, alpha=0.8)
    ax.set_yticks(range(len(top_states)))
    ax.set_yticklabels(top_states['cal_state'])
    ax.set_title("Top CAL States by Message Volume",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_ylabel("CAL State", fontweight='bold', fontsize=10)
    ax.grid(True, alpha=0.3, axis='x')

    # Add value labels
    for i, (idx, row) in enumerate(top_states.iterrows()):
        ax.text(row['message_count'] + max(top_states['message_count']) * 0.01,
                i, f"{row['message_count']:,}",
                va='center', fontsize=9)

    ax.invert_yaxis()
else:
    ax.text(0.5, 0.5, "No CAL state data available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

The distribution shows which learning states students spend the most time in, indicating where the AI is most frequently engaged.

---

# Part 3: Sentiment Analysis {.tabset}

Sentiment analysis examines the emotional tone of messages exchanged between students and the AI assistant. Understanding sentiment helps assess user satisfaction and the quality of AI responses.

## Understanding Sentiment Analysis

**What is being measured?**

Messages are analyzed for emotional tone and classified into sentiment categories (typically Positive, Neutral, and Negative). This helps us understand:

- **User sentiment**: How students feel when interacting with the AI
- **AI sentiment**: Whether the AI maintains an appropriate, supportive tone
- **Interaction quality**: The overall emotional dynamic of conversations

**Technical methodology:**

The sentiment analysis is performed using transformer-based natural language processing models. Each message is processed through the following pipeline:

1. **Text preprocessing**: Messages are cleaned and normalized, preserving emotionally significant elements like punctuation and capitalization
2. **Model inference**: A pre-trained sentiment classification model (typically BERT-based or similar transformer architecture) analyzes the text and assigns probability scores to each sentiment category
3. **Classification**: Messages are labeled as Positive, Neutral, or Negative based on the highest probability score
4. **Aggregation**: Results are grouped by message role (user vs. assistant) to compare sentiment patterns

The model has been fine-tuned on conversational text data to better capture the nuances of educational interactions, including informal language, code snippets, and domain-specific terminology common in learning platforms.

## 3.1 Sentiment Distribution

```{python}
#| label: tbl-sentiment-counts
#| tbl-cap: "Sentiment distribution by message role"
from IPython.display import display, Markdown

sentiment_long = pd.concat([
    sentiment_user.assign(role="user"),
    sentiment_ai.assign(role="assistant"),
], ignore_index=True, sort=False)

sentiment_counts = (
    sentiment_long.groupby(["role", "sentiment_label"]).size().reset_index(name="messages")
)

if not sentiment_counts.empty:
    # Calculate percentages
    sentiment_display = sentiment_counts.copy()
    sentiment_display['percentage'] = sentiment_display.groupby('role')['messages'].transform(
        lambda x: (x / x.sum() * 100).round(1)
    )
    sentiment_display.columns = ['Role', 'Sentiment', 'Count', 'Percentage (%)']
    sentiment_display['Role'] = sentiment_display['Role'].str.capitalize()
    display(Markdown(sentiment_display.to_markdown(index=False)))
else:
    print("No sentiment data available")
```

## 3.2 Sentiment Comparison

```{python}
#| label: fig-sentiment-comparison
#| fig-cap: "Sentiment distribution comparison between user and AI messages"
fig, ax = plt.subplots(figsize=(8, 4.5))
if not sentiment_counts.empty:
    # Map sentiment labels to colors
    sentiment_colors = {
        'Positive': COLOR_POSITIVE,
        'Neutral': COLOR_NEUTRAL,
        'Negative': COLOR_NEGATIVE,
        'positive': COLOR_POSITIVE,
        'neutral': COLOR_NEUTRAL,
        'negative': COLOR_NEGATIVE
    }

    # Prepare data for plotting
    plot_data = sentiment_counts.copy()
    plot_data['role'] = plot_data['role'].str.capitalize()

    # Create grouped bar chart
    x = np.arange(len(plot_data['sentiment_label'].unique()))
    width = 0.35
    user_data = plot_data[plot_data['role'] == 'User']
    ai_data = plot_data[plot_data['role'] == 'Assistant']

    ax.bar(x - width/2, user_data['messages'], width, label='User',
           color=COLOR_USER, alpha=0.8)
    ax.bar(x + width/2, ai_data['messages'], width, label='Assistant',
           color=COLOR_AI, alpha=0.8)

    ax.set_title("Sentiment Distribution by Role",
                 fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
    ax.set_xlabel("Sentiment", fontweight='bold', fontsize=10)
    ax.set_ylabel("Number of Messages", fontweight='bold', fontsize=10)
    ax.set_xticks(x)
    ax.set_xticklabels(user_data['sentiment_label'].str.capitalize())
    ax.legend(title='Role', fontsize=10, title_fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Add value labels
    for container in ax.containers:
        ax.bar_label(container, fontsize=9)
else:
    ax.text(0.5, 0.5, "No sentiment records available",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```

This comparison reveals whether users and the AI assistant maintain different emotional tones during interactions.

## 3.3 Overall Sentiment Summary

```{python}
#| label: tbl-sentiment-overview
#| tbl-cap: "Overall sentiment statistics"
from IPython.display import display, Markdown

if not sentiment_overview.empty:
    display(Markdown(sentiment_overview.to_markdown(index=False)))
else:
    print("No sentiment overview data available")
```

---

# Part 4: Topic Modeling {.tabset}

Topic modeling identifies the main themes and subjects discussed in conversations. This analysis reveals what students talk about most frequently and how the AI responds. 

## Understanding Topic Modeling

**What is topic modeling?**

Topic modeling uses natural language processing to automatically discover themes in text data. Each topic is represented by:

- **Keywords**: The most characteristic words for that topic
- **Message count**: How many messages belong to that topic
- **Role**: Whether the topic appears in user or AI messages

This helps identify common student questions, concerns, and the AI's response patterns.

**Technical methodology:**

The topic modeling is performed using BERTopic, a modern topic modeling technique that leverages transformer embeddings and clustering algorithms:

1. **Text embedding**: Each message is converted into a high-dimensional semantic vector using sentence transformers (SBERT). These embeddings capture the contextual meaning of the text, not just keyword frequency
2. **Dimensionality reduction**: UMAP (Uniform Manifold Approximation and Projection) reduces the embedding dimensions while preserving semantic relationships, making clusters more identifiable
3. **Clustering**: HDBSCAN (Hierarchical Density-Based Spatial Clustering) groups semantically similar messages together based on their embeddings. Unlike traditional methods like LDA, this approach doesn't require pre-specifying the number of topics
4. **Topic representation**: For each cluster, c-TF-IDF (class-based Term Frequency-Inverse Document Frequency) extracts the most representative keywords that distinguish one topic from others
5. **OpenAI refinement (optional)**: When enabled, raw keywords and sample messages from each topic are sent to OpenAI's GPT models (e.g., gpt-4o-mini) to generate human-readable topic labels. This produces more natural, interpretable descriptions compared to keyword lists alone
6. **Role separation**: Topics are analyzed separately for user messages and AI messages to understand the conversation dynamics from both perspectives

This approach provides more coherent and interpretable topics compared to traditional methods, especially for short conversational messages where context is crucial.

**How to interpret OpenAI-refined topics:**

When OpenAI representation is enabled (using the `--use-openai-representation` flag), the topic labels you see are **not** raw statistical keywords. Instead, they are natural language interpretations generated by a large language model (GPT) based on:

- The initial keyword list from c-TF-IDF
- Sample messages from the topic cluster
- Context about the domain and conversation type

**What this means for interpretation:**

- **More readable**: Topics appear as coherent phrases rather than disconnected keywords (e.g., "Business model validation strategies" instead of "business, model, validate, customer, market")
- **Higher-level abstraction**: The LLM synthesizes the underlying theme rather than just listing common terms
- **Consistency**: Using temperature=0.0 ensures deterministic, consistent labeling across runs
- **Trade-offs**: While more interpretable, OpenAI labels may occasionally oversimplify or miss nuances present in the raw keywords. For critical analysis, review both the OpenAI label and the underlying keyword distribution

**When OpenAI refinement is NOT used** (default behavior), you'll see raw keyword lists extracted via c-TF-IDF, which represent the most statistically distinctive terms for each topic. These are more granular but require more manual interpretation to understand the broader theme.

## 4.1 User Message Topics

::: {.figure-scroll}
```{python}
#| label: fig-user-topics
#| fig-cap: "Top 10 topics in user messages"
fig, ax = plt.subplots(figsize=(12, 8))
if not topic_keywords.empty:
    # Get top 10 topics for user messages
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False).head(10)

    if not user_topics.empty:
        # Sort by message count for better visualization
        user_topics = user_topics.sort_values('message_count', ascending=True)

        # Create horizontal bar chart
        y_pos = np.arange(len(user_topics))
        bars = ax.barh(y_pos, user_topics['message_count'], color=COLOR_USER, alpha=0.8)
        ax.set_yticks(y_pos)
        # Use intent_label if available, otherwise fall back to keywords
        labels = user_topics['intent_label'] if 'intent_label' in user_topics.columns else user_topics['keywords']
        ax.set_yticklabels(labels, fontsize=9)
        ax.set_title("Top Topics in User Messages",
                     fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
        ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
        ax.set_ylabel("Topic", fontweight='bold', fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (idx, row) in enumerate(user_topics.iterrows()):
            ax.text(row['message_count'] + max(user_topics['message_count']) * 0.01,
                    i, f"{row['message_count']:,}",
                    va='center', fontsize=8)
    else:
        ax.text(0.5, 0.5, "No user topics identified",
                ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
        ax.set_xticks([])
        ax.set_yticks([])
else:
    ax.text(0.5, 0.5, "Topic modeling did not return any clusters",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```
:::

The user topic visualization reveals what students are asking about and discussing most frequently. These topics represent the primary concerns, questions, and areas where students seek assistance from the AI.

## 4.2 AI Message Topics

::: {.figure-scroll}
```{python}
#| label: fig-ai-topics
#| fig-cap: "Top 10 topics in AI messages"
fig, ax = plt.subplots(figsize=(12, 8))
if not topic_keywords.empty:
    # Get top 10 topics for AI messages
    ai_topics = topic_keywords[topic_keywords['message_role'] == 'assistant'].sort_values('message_count', ascending=False).head(10)

    if not ai_topics.empty:
        # Sort by message count for better visualization
        ai_topics = ai_topics.sort_values('message_count', ascending=True)

        # Create horizontal bar chart
        y_pos = np.arange(len(ai_topics))
        bars = ax.barh(y_pos, ai_topics['message_count'], color=COLOR_AI, alpha=0.8)
        ax.set_yticks(y_pos)
        # Use intent_label if available, otherwise fall back to keywords
        labels = ai_topics['intent_label'] if 'intent_label' in ai_topics.columns else ai_topics['keywords']
        ax.set_yticklabels(labels, fontsize=9)
        ax.set_title("Top Topics in AI Messages",
                     fontweight='bold', fontsize=14, color=COLOR_PRIMARY)
        ax.set_xlabel("Number of Messages", fontweight='bold', fontsize=10)
        ax.set_ylabel("Topic", fontweight='bold', fontsize=10)
        ax.grid(True, alpha=0.3, axis='x')

        # Add value labels
        for i, (idx, row) in enumerate(ai_topics.iterrows()):
            ax.text(row['message_count'] + max(ai_topics['message_count']) * 0.01,
                    i, f"{row['message_count']:,}",
                    va='center', fontsize=8)
    else:
        ax.text(0.5, 0.5, "No AI topics identified",
                ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
        ax.set_xticks([])
        ax.set_yticks([])
else:
    ax.text(0.5, 0.5, "Topic modeling did not return any clusters",
            ha="center", va="center", fontsize=12, color=COLOR_NEUTRAL)
    ax.set_xticks([])
    ax.set_yticks([])
plt.tight_layout()
plt.show()
```
:::

The AI topic visualization shows the main themes in the assistant's responses. These topics reflect how the AI is addressing student needs and what types of support it provides most frequently.

## 4.3 Interpreting Topic Patterns

To ground the topic labels, the tables below show real message excerpts with topic keywords highlighted. This makes interpretations like “Casual affirmations” tangible by displaying the language patterns that define each cluster.

Notes:

- Highlights are derived from each topic’s keyword list. Because messages are lemmatized during preprocessing, highlighted forms may include closely related surface variants.
- Examples come directly from the translated messages (the English text), giving a clear view of the student or AI phrasing that BERTopic grouped together.

### User topic examples

```{python}
from IPython.display import HTML, display

if not topic_keywords.empty and not message_topics_user.empty:
    top_user = (
        topic_keywords[topic_keywords['message_role'] == 'user']
        .sort_values('message_count', ascending=False)
        .head(5)
    )
    rows = sample_topic_examples(message_topics_user, top_user, per_topic=3)

    html_parts = [
        "<table class='table table-sm'>",
        "<thead><tr><th style='width:20%'>Topic</th><th style='width:30%'>Keywords</th><th>Example messages</th></tr></thead>",
        "<tbody>",
    ]
    for row in rows:
        examples_html = "<ul>" + "".join(f"<li>{example}</li>" for example in row['examples']) + "</ul>"
        topic_display = f"{row['topic_id']}: {row['intent_label']}"
        html_parts.append(
            f"<tr><td><b>{topic_display}</b></td><td><i>{row['keywords']}</i></td><td>{examples_html}</td></tr>"
        )
    html_parts.append("</tbody></table>")
    display(HTML("\n".join(html_parts)))
else:
    print("No user topic examples available.")
```

### AI topic examples

```{python}
from IPython.display import HTML, display

if not topic_keywords.empty and not message_topics_assistant.empty:
    top_ai = (
        topic_keywords[topic_keywords['message_role'] == 'assistant']
        .sort_values('message_count', ascending=False)
        .head(5)
    )
    rows = sample_topic_examples(message_topics_assistant, top_ai, per_topic=3)

    html_parts = [
        "<table class='table table-sm'>",
        "<thead><tr><th style='width:20%'>Topic</th><th style='width:30%'>Keywords</th><th>Example messages</th></tr></thead>",
        "<tbody>",
    ]
    for row in rows:
        examples_html = "<ul>" + "".join(f"<li>{example}</li>" for example in row['examples']) + "</ul>"
        topic_display = f"{row['topic_id']}: {row['intent_label']}"
        html_parts.append(
            f"<tr><td><b>{topic_display}</b></td><td><i>{row['keywords']}</i></td><td>{examples_html}</td></tr>"
        )
    html_parts.append("</tbody></table>")
    display(HTML("\n".join(html_parts)))
else:
    print("No AI topic examples available.")
```

**Topic Alignment and Pedagogical Insights:**

The relationship between user and AI topics reveals the quality of conversational alignment:

- **Responsive teaching**: When AI topics mirror user topics with added depth, it indicates the assistant is directly addressing student needs
- **Proactive guidance**: Topics in AI messages that extend beyond immediate user questions suggest the assistant is providing broader context and anticipatory support
- **Knowledge gaps**: Frequent topics about basic concepts may indicate areas where students need more foundational instruction or clearer explanations
- **Engagement patterns**: The diversity and specificity of topics reflect how deeply students are engaging with course material

By analyzing these patterns over time, educators can identify which topics require enhanced content, where students struggle most, and how to optimize the AI's pedagogical approach.

---

# Summary and Recommendations

## Key Insights

Based on the analysis of student-AI interactions, several important patterns emerge:

```{python}
#| output: asis
insights = []

# Message activity insights
if not timeline_daily.empty and not timeline_hourly.empty:
    peak_hour = timeline_hourly.loc[timeline_hourly['messages'].idxmax(), 'hour']
    total_messages = timeline_daily['messages'].sum()
    insights.append(f"**Engagement Patterns**: Students sent {total_messages:,} messages, with peak activity occurring around hour {peak_hour}.")

# CAL state insights
if not cal_state_counts.empty:
    top_state = cal_state_counts.iloc[0]
    insights.append(f"**Learning States**: The '{top_state['cal_state']}' state generated the most engagement with {top_state['message_count']:,} messages.")

# Sentiment insights
if not sentiment_counts.empty:
    user_sentiments = sentiment_counts[sentiment_counts['role'] == 'user']
    if not user_sentiments.empty:
        dominant_sentiment = user_sentiments.loc[user_sentiments['messages'].idxmax(), 'sentiment_label']
        insights.append(f"**User Sentiment**: Students predominantly express {dominant_sentiment} sentiment in their interactions.")

# Topic insights
if not topic_keywords.empty:
    user_topics = topic_keywords[topic_keywords['message_role'] == 'user'].sort_values('message_count', ascending=False)
    if not user_topics.empty:
        top_topic = user_topics.iloc[0]
        insights.append(f"**Common Topics**: The most frequent discussion topic among students is '{top_topic['keywords']}' with {top_topic['message_count']} messages.")

for insight in insights:
    print(f"{insight}\n\n")
```

## Recommendations

Based on these findings, we recommend the following actions:

1. **Optimize Engagement Timing**
   - Schedule proactive AI interventions during peak usage hours
   - Consider sending reminders or notifications when students are most active

2. **Enhance High-Volume CAL States**
   - Focus development efforts on improving the most-used learning states
   - Add more content and support for popular interaction contexts

3. **Address Sentiment Patterns**
   - Monitor negative sentiment trends and identify root causes
   - Refine AI responses to maintain positive, supportive interactions

4. **Expand Topic Coverage**
   - Create targeted content for frequently discussed topics
   - Develop specialized responses for common student questions

5. **Data Quality and Monitoring**
   - Refresh the dataset regularly by re-running the analysis pipeline
   - Monitor sentiment shifts over time as new cohorts are onboarded
   - Conduct qualitative review of high-volume topics to improve content

## Next Steps

To continue improving the analysis:

- **Update Data**: Re-run the Snakemake pipeline when new data becomes available
- **Deeper Analysis**: Conduct user interviews to validate automated findings
- **A/B Testing**: Experiment with different AI response strategies based on insights
- **Longitudinal Tracking**: Monitor how patterns change over time

---

```{python}
#| output: asis
print(f"**Report generated:** {pd.Timestamp.today().strftime('%B %d, %Y')}")
print(f"\n**Analysis period:** {datetime_overview.get('start_datetime', 'N/A')} to {datetime_overview.get('end_datetime', 'N/A')}")
print(f"\n**Total messages analyzed:** {datetime_overview.get('total_messages', 'N/A'):,}")
```
